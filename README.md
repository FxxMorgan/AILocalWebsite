# ğŸ¤– AI Chat Local

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D%2016.0.0-brightgreen.svg)](https://nodejs.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)
[![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)](https://github.com/FxxMorgan)
[![LMStudio Compatible](https://img.shields.io/badge/LMStudio-Compatible-blue.svg)](https://lmstudio.ai/)

> Cliente web elegante y ligero para interactuar con modelos de IA locales a travÃ©s de LMStudio

**Desarrollado con â¤ï¸ por [Feer](https://github.com/FxxMorgan)**

## âœ¨ CaracterÃ­sticas

- ğŸ¨ **Interfaz moderna** - DiseÃ±o responsivo con tema oscuro/claro
- ğŸ’¬ **Chat inteligente** - Conversaciones fluidas con modelos locales
- ğŸ“± **MÃ³vil optimizado** - Experiencia perfecta en dispositivos mÃ³viles
- ï¿½ **GestiÃ³n de chats** - Guarda, edita y organiza conversaciones
- ğŸ› ï¸ **ConfiguraciÃ³n avanzada** - Control de temperatura, tokens, modelos
- ğŸ”Œ **Compatibilidad total** - Funciona con cualquier modelo en LMStudio
- ï¿½ **Reintentos automÃ¡ticos** - Manejo robusto de errores y timeouts
- ï¿½ **Copia de cÃ³digo** - Botones para copiar bloques de cÃ³digo
- ğŸ¯ **Fallbacks inteligentes** - Soporte para modelos sin formato chat

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- [Node.js](https://nodejs.org/) v16 o superior
- [LMStudio](https://lmstudio.ai/) ejecutÃ¡ndose localmente
- Un modelo descargado en LMStudio

### InstalaciÃ³n

1. **Clona el repositorio**
   ```bash
   git clone https://github.com/FxxMorgan/AI-Chat-Local.git
   cd AI-Chat-Local
   ```

2. **Instala dependencias**
   ```bash
   npm install
   ```

3. **Configura el entorno** (opcional)
   ```bash
   cp .env.example .env
   # Edita .env con tus preferencias
   ```

4. **Configura LMStudio**
   - Abre LMStudio
   - Ve a la pestaÃ±a "Local Server"
   - Carga tu modelo preferido (recomendado: qwen/qwen3-4b-2507)
   - AsegÃºrate de que el servidor estÃ© habilitado en puerto 1234

5. **Inicia el servidor**
   ```bash
   npm start
   ```

6. **Abre en tu navegador**
   ```
   http://localhost:3000
   ```
## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
# Puerto del servidor
PORT=3000

# URL de LMStudio
LMSTUDIO_URL=http://localhost:1234

# ConfiguraciÃ³n por defecto
DEFAULT_MODEL=qwen/qwen3-4b-2507
DEFAULT_MAX_TOKENS=500
DEFAULT_TEMPERATURE=0.7

# ConfiguraciÃ³n avanzada
LM_TIMEOUT_MS=60000
LM_MAX_RETRIES=2
VERBOSE_LM=false
WARMUP_ON_START=false

# Modelos que requieren formato raw (separados por comas)
FORCE_RAW_MODELS=nsfw-3b,otro-modelo*
```

## ğŸ“– Uso

### Interfaz Principal

- **Chat principal**: EnvÃ­a mensajes y recibe respuestas del modelo
- **Barra lateral**: Gestiona conversaciones y ajusta configuraciones
- **Configuraciones**: Modifica temperatura, tokens mÃ¡ximos y modelo activo

### GestiÃ³n de Conversaciones

- **Nueva conversaciÃ³n**: BotÃ³n `+` en la barra lateral
- **Cambiar conversaciÃ³n**: Click en cualquier chat de la lista
- **Editar tÃ­tulo**: BotÃ³n de ediciÃ³n en cada conversaciÃ³n
- **Eliminar**: BotÃ³n de papelera (individual o todas)

### Atajos de Teclado

- `Enter`: Enviar mensaje
- `Shift + Enter`: Nueva lÃ­nea
- `Escape`: Cerrar barra lateral (mÃ³vil)

## ğŸ› ï¸ API Endpoints

### Chat
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Hola, Â¿cÃ³mo estÃ¡s?",
  "model": "qwen/qwen3-4b-2507",
  "max_tokens": 300,
  "temperature": 0.7,
  "chatId": "optional-chat-id"
}
```

### GestiÃ³n de Modelos
```http
GET /api/models              # Listar modelos disponibles
GET /api/status              # Estado del servidor y LMStudio
```

### GestiÃ³n de Chats
```http
GET /api/chats               # Listar todas las conversaciones
POST /api/chats              # Crear nueva conversaciÃ³n
GET /api/chats/:id           # Obtener conversaciÃ³n especÃ­fica
PUT /api/chats/:id           # Actualizar tÃ­tulo
DELETE /api/chats/:id        # Eliminar conversaciÃ³n
DELETE /api/chats            # Eliminar todas las conversaciones
```

## ğŸ¨ PersonalizaciÃ³n

### Temas

El proyecto incluye soporte para tema oscuro/claro automÃ¡tico basado en las preferencias del sistema. Puedes personalizar los colores editando las variables CSS en `public/index.html`.

### Modelos Personalizados

Para aÃ±adir soporte para modelos especÃ­ficos que requieren formatos especiales:

1. AÃ±ade el modelo a `FORCE_RAW_MODELS` en `.env`
2. El sistema automÃ¡ticamente usarÃ¡ fallbacks apropiados

## ğŸ¤ Contribuir

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### Desarrollo Local

```bash
# Modo desarrollo con auto-reload
npm run dev

# Ejecutar tests (si existen)
npm test

# Verificar linting
npm run lint
```

## ğŸ“ Estructura del Proyecto

```
AI-Chat-Local/
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html          # Interfaz principal
â”‚   â”œâ”€â”€ script.js           # LÃ³gica del frontend
â”‚   â””â”€â”€ style.css           # Estilos (legacy)
â”œâ”€â”€ chats/                  # Almacenamiento de conversaciones
â”œâ”€â”€ server.js               # Servidor Express
â”œâ”€â”€ chat-storage.js         # Sistema de persistencia
â”œâ”€â”€ package.json            # Dependencias
â”œâ”€â”€ .env.example            # ConfiguraciÃ³n de ejemplo
â””â”€â”€ README.md               # Este archivo
```

## ğŸ› SoluciÃ³n de Problemas

### LMStudio no conecta

1. Verifica que LMStudio estÃ© ejecutÃ¡ndose
2. Confirma que el servidor local estÃ© habilitado
3. Verifica la URL en `LMSTUDIO_URL`

### Timeouts frecuentes

1. Aumenta `LM_TIMEOUT_MS` en `.env`
2. Habilita `WARMUP_ON_START=true`
3. Reduce `max_tokens` para respuestas mÃ¡s rÃ¡pidas

### Modelo no funciona

1. AÃ±ade el modelo a `FORCE_RAW_MODELS`
2. Verifica que estÃ© cargado en LMStudio
3. Revisa los logs del servidor para detalles

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Feer** - [@FxxMorgan](https://github.com/FxxMorgan)

## ğŸ™ Agradecimientos

- [LMStudio](https://lmstudio.ai/) por la increÃ­ble plataforma
- [Tailwind CSS](https://tailwindcss.com/) por el sistema de diseÃ±o
- [Font Awesome](https://fontawesome.com/) por los iconos
- Comunidad open source por la inspiraciÃ³n

---

<div align="center">
  <strong>â­ Â¡Dale una estrella si te gusta el proyecto! â­</strong>
</div>
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Desarrollador

**Feer** - [GitHub](https://github.com/FxxMorgan)

Si te gusta este proyecto, Â¡no olvides darle una â­!

---

*Desarrollado con â¤ï¸ usando Node.js, Express, Tailwind CSS y LMStudio*
